---
title: "[airflow] docker로 airflow 설치 및 사용"
categories:
 - airflow
excerpt: " "
tags:
 - airflow
 - data engineer
 - dataflow
 - ETL
 - docker
---
## 0. 환경
- docker OS: ubuntu
- airflow version: airflow 2.1.4
- metaDB: mysql 5.7

## 1. docker container에 airflow 설치하기
- docker ubuntu 위에 airflow 설치 및 config 설정
```bash
$ docker run -it -p 1044:8080 -p 1033:22 --name airflow_test ubuntu
$ docker exec -it airflow_test /bin/bash

$ apt-get update -y
$ apt-get install -y vim
$ apt-get install -y python3-pip

$ apt-get install -y libmysqlclient-dev
$ pip3 install numpy
$ pip3 install pandas
$ pip3 install apache-airflow==2.1.4
$ pip3 install typing_extensions

# mysql을 target으로 설정
$ pip3 install mysqlclient

# 우분투 한글 설정
$ apt-get -y install language-pack-ko
$ apt-get update
$ locale-gen ko_KR.UTF-8
$ dpkg-reconfigure locales
$ Locales to be generated: ko_KR.UTF-8 UTF8 선택
$ Default locale for the system environment: ko_KR.UTF-8 선택
$ export LANGUAGE=ko_KR.UTF-8
$ export LANG=ko_KR.UTF-8
# locale로 제대로 바뀌었는지 확인
# .bashrc에 LANGUAGE, LANG 기록하여 자동 적용

# TB_NLS_LANG 설정
$ export TB_NLS_LANG=UTF8

# airflow 첫구동을 위한 db 초기화
$ airflow db init

# 아래와 같이 설정 변경
$ vi airflow.cfg
default_timezone = Asia/Seoul
sql_alchemy_conn = mysql://root:mysql@192.168.179.45:1098/airflow?charset=utf8mb4
load_examples = False

# mysql 등 적용을 위한 db 다시 초기화
$ airflow db init

# airflow 유저 생성
$ airflow users create \
--username admin \
--firstname jason \
--lastname lee \
--role Admin \
--email admin

# odbc 설정
# libtbodbc.so 파일 가져오기
$ apt-get install ssh
$ sftp root@192.168.179.46
$TB_HOME/client/lib/libtbodbc.so 파일 가져오기

$ apt-get install -y unixodbc unixodbc-dev
$ pip3 install pyodbc

# pyodbc 설치 후 DSN 설정
$ vi /etc/odbc.ini
[ODBC Data Sources]
Tibero6 = Tibero6 ODBC driver
[ODBC]
Trace = 1
TraceFile = /tmp/odbc.trace
[tibero6] # DSN 이름
Driver = /root/airflow/libtbodbc.so
Description = Tibero6 ODBC Datasource
# server는 tody 서버
server = 192.168.179.47
port = 8629
# database는 DB_NAME
database = tibero
User = biqa
Password = tmax

$ vi /etc/odbcinst.ini
[tibero6]
Description = Tibero6 ODBC driver
Driver = /root/airflow/libtbodbc.so
Setup = /root/airflow/libtbodbc.so
Setup = 1
```

## 2. airflow 사용 관련 내용
- airflow 유저 생성  
  `airflow users create --username admin --firstname jason --lastname lee --role Admin --email admin`
- airflow.cfg 변경하고 airflow db upgrade 수행해야 변경 내용 적용됨.
- default_args: task level에 적용되는 파라미터
  - 'owner', 'email', 'retries', 'retry_delay' 등이 있음.
- DAG(): dag level에 적용되는 파라미터
  - 'start_date', 'schedule_interval', 'default_arg' 등이 있음.
  - default_arg는 위에 설정한 default_args를 의미
  - 특히, 'max_active_runs', 'concurrency', 'catchup'은 task level 파라미터가 아닌 dag level 파라미터
- default_args에 dag 파라미터 넣어도 에러는 안나지만 적용이 안됨.
- dag_dir_list_interval = 300
  - How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
  - 얼마나 자주 dag 폴더를 스캔할지 설정이며, 기본이 5분이고 단위는 초 단위임.
- SQL 작성할 때 에러 방지를 위한 rollback 사용을 위해 웬만하면 truncate/drop 대신 delete 사용하기
- autocommit 쓰지 말고 직접 상황 나눠서 commit/rollback 작성(데이터 보호 위해)  
- airflow meta DB인 airflow in mysql의 캐릭터셋이 utf8mb여야 utf-8 문자열 insert 가능
  - "CREATE DATABASE airflow CHARACTER SET utf8mb"로 db 재생성하고 airflow db init 하는 방법
  - 그냥 db 캐릭터셋 utf8mb로 바꿔주는 방법  
- AirFlow의 BackFill을 이용하여 작업을 수행하다가, 어떤 task에서 Failure가 발생했다고 하자.
이 때, 해당 task의 뒤쪽 즉, 이 task가 성공해야 수행될 수 있는 task들은 모두 upstream_failure라는 실패 상태가 된다.
이렇게 task의 failure가 발생했을 때, failure의 발생원인을 찾아서 다시 실행한다고 해도, 한번 실패했던 내용은 계속해서 실패하게 된다.
이것이 AirFlow 측에서 의도한 것인지는 정확히 모르겠으나, 문제 원인을 해결했으면 문제가 발생한 task들의 실패기록을 지워주어야 재실행할 수 있다.
다음과 같이 Task들이 순서대로 실행되는 DAG가 있었다고 가정하자.  
`A->B->C->D->E`  
이 때, C에서 문제가 발생했다면 C 이후의 D,E에 대해서도 실패기록을 지워줘야 한다.
이때는 airflow의 CLI help 명령어에 따라 다음과 같이 명령어를 주면 C와 그 downstream들의 실패기록을 삭제할 수 있다.  
이렇게 실패 기록을 지워주면 backfill 등으로 재실행 했을때, 실패했던 C부터 작업을 진행하게 된다.  
출처: https://monkeydev.tistory.com/2
  ```
  (airflowtest) sungil@sungil:~$ airflow clear --help
  [2019-06-08 10:46:49,826] {__init__.py:51} INFO - Using executor SequentialExecutor
  usage: airflow clear [-h] [-t TASK_REGEX] [-s START_DATE] [-e END_DATE]
                      [-sd SUBDIR] [-u] [-d] [-c] [-f] [-r] [-x] [-xp] [-dx]
                      dag_id

  positional arguments:
    dag_id                The id of the dag

  optional arguments:
    -h, --help            show this help message and exit
    -t TASK_REGEX, --task_regex TASK_REGEX
                          The regex to filter specific task_ids to backfill
                          (optional)
    -s START_DATE, --start_date START_DATE
                          Override start_date YYYY-MM-DD
    -e END_DATE, --end_date END_DATE
                          Override end_date YYYY-MM-DD
    -sd SUBDIR, --subdir SUBDIR
                          File location or directory from which to look for the
                          dag. Defaults to '[AIRFLOW_HOME]/dags' where
                          [AIRFLOW_HOME] is the value you set for 'AIRFLOW_HOME'
                          config you set in 'airflow.cfg'
    -u, --upstream        Include upstream tasks
    -d, --downstream      Include downstream tasks
    -c, --no_confirm      Do not request confirmation
    -f, --only_failed     Only failed jobs
    -r, --only_running    Only running jobs
    -x, --exclude_subdags

  (airflowtest) sungil@sungil:~$ airflow clear -t <C의 task name> --downstream <c가 속한 DAG name>
  ```  

- airflow에서 exit() 등 프로그램 종료 구문을 사용하면 fail로 인식함.

## 3. 참고사항
- [airflow의 metaDB를 mysql로 바꾸는 방법](https://dydwnsekd.tistory.com/39)
- [VScode로 remote server docker container에 ssh로 붙어서 작업하는 법](https://data-newbie.tistory.com/688)
- [버킷플레이스 airflow 도입기](https://www.bucketplace.co.kr/post/2021-04-13-%EB%B2%84%ED%82%B7%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-airflow-%EB%8F%84%EC%9E%85%EA%B8%B0/)

- 데이터 마트 vs 데이터 웨어하우스
  ![2번](/assets/airflow/구조2)
- 데이터 아키텍처
  ![1번](/assets/airflow/구조1)

- python에서 mysql 사용하기
  ```python
  import pymysql
  my_conn = pymysql.connect(user = 'root', passwd = 'mysql', host = '192.168.179.47', port = 1088, db = 'datamart', charset = 'utf8')
  my_cur = my_conn.cursor()
  ```
- python에서 SCP 사용해서 파일 가져오기
    ```python
    # how to read or get files from remote server
    import paramiko
    from scp import SCPClient, SCPException
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect('192.168.179.31', username='root', password='tmaxbi1234!', allow_agent=False)
    stdin, stdout, stderr = ssh.exec_command('ls')
    stdout.read()
    sftp_client = ssh.open_sftp()
    remote_file = sftp_client.open('/root/biqa/installer/src/hd/log_release20.4_20220621_1339.txt')
    SCPClient(ssh.get_transport()).get('/root/biqa/installer/src/hd/log_release20.4_20220621_1339.txt', '/root/airflow/text.txt')
    with SCPClient(ssh.get_transport()) as scp:
        scp.get('/root/biqa/installer/src/hd/log_release20.4_20220621_1339.txt', '/root/airflow/text.txt')
    -- 와일드카드 사용하기
    SCPClient(ssh.get_transport(), sanitize=lambda x: x).get('/root/biqa/installer/src/hd/log_release*', '/root/airflow/')
    ------------------------------------------------------------------------------------------------------------
    def create_ssh_client(self, hostname, username, password):
        """Create SSH client session to remote server"""
        if self.ssh_client is None:
            self.ssh_client = paramiko.SSHClient()
            self.ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            self.ssh_client.connect(hostname, username=username, password=password)
        else:
            print("SSH client session exist.")

    def close_ssh_client(self):
        """Close SSH client session"""
        self.ssh_client.close()

    def send_file(self, local_path, remote_path):
        """Send a single file to remote path"""
        try:
            with SCPClient(self.ssh_client.get_transport()) as scp:
                scp.put(local_path, remote_path, preserve_times=True)
        except SCPException:
            raise SCPException.message

    def get_file(self, remote_path, local_path):
        """Get a single file from remote path"""
        try:
            with SCPClient(self.ssh_client.get_transport()) as scp:
                scp.get(remote_path, local_path)
        except SCPException:
            raise SCPException.message
            
    def send_command(self, command):
        """Send a single command"""
        stdin, stdout, stderr = self.ssh_client.exec_command(command)
        return stdout.readlines()
    ```
- docker 데이터 영속 보관을 위해 호스트에 폴더를 생성하여 연동해야 함.
  - docker는 컨테이너가 삭제되면 컨테이너에 쓰여진 데이터가 다 날라가기 때문에 별도 호스트에 volume을 생성해줘야 한다.
  - 2가지 방법이 있는데 docker volume 생성 또는 bind mount가 있다.
  - docker volume은 docker 명령어로 생성하는 docker 자체 volume이므로 docker로 관리되고 디테일한 정보를 docker volume inspect ${container_name}으로 쉽게 볼 수 있다.
  - bind mount는 직접 mkdir로 생성한 디렉토리를 volume으로 사용하는 것이다. 원하는 위치에 생성하여 관리가 가능하다.
  - 두가지 모두 아래 형태로 사용이 가능함.
  - docker run -it -p 1234:8080 -v /root/dags_test:/root/airflow/dags_test --name airflow_jason_v airflow_jason2 /bin/bash

## 4. airflow docs에서 유용한 내용 기록
https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html  
<b>Creating task for the best</b>
- Do not use INSERT during a task re-run, an INSERT statement might lead to duplicate rows in your database. Replace it with UPSERT.
  - upsert: An operation that inserts rows into a database table if they do not already exist, or updates them if they do.
  - an upsert is a database operation that will update an existing row if a specified value already exists in a table, and insert a new row if the specified value doesn't already exist.
  - CREATE a TEMPORARY table. -> COPY or bulk-insert the new data into the temp table.
- Read and write in a specific partition. Never read the latest available data in a task. Someone may update the input data between re-runs, which results in different outputs. A better way is to read the input data from a specific partition. You can use data_interval_start as a partition. You should follow this partitioning method while writing data in S3/HDFS as well.
- The Python datetime now() function gives the current datetime object. This function should never be used inside a task, especially to do the critical computation, as it leads to different outcomes on each run. It’s fine to use it, for example, to generate a temporary log.

<b>Avoid writing Top level Python Code for best(import한 lib가 특정 def 내에서만 쓰인다면 특정 def 안에서만 lib import 하기)</b>
- You should avoid writing the top level code which is not necessary to create Operators and build DAG relations between them. This is because of the design decision for the scheduler of Airflow and the impact the top-level code parsing speed on both performance and scalability of Airflow.
- Airflow scheduler executes the code outside the Operator’s execute methods with the minimum interval of min_file_process_interval seconds. This is done in order to allow dynamic scheduling of the DAGs - where scheduling and dependencies might change over time and impact the next schedule of the DAG. Airflow scheduler tries to continuously make sure that what you have in DAGs is correctly reflected in scheduled tasks.
- Specifically you should not run any database access, heavy computations and networking operations.
- One of the important factors impacting DAG loading time, that might be overlooked by Python developers is that top-level imports might take surprisingly a lot of time and they can generate a lot of overhead and this can be easily avoided by converting them to local imports inside Python callables for example.
- Consider the example below - the first DAG will parse significantly slower (in the orders of seconds) than equivalent DAG where the numpy module is imported as local import in the callable.  